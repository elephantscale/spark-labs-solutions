<link rel='stylesheet' href='../assets/css/main.css'/>


# Lab 5.1 First Spark Job Submission

### Overview

Submit a job for Spark

### Depends On

None

### Run time

20-30 mins


## STEP 1: Be in Project Dir

Go to the project root directory

```bash
    $    cd ~/dev/spark-labs/05-api
```

## Step 2 - Edit File

**=> Edit file : `~/dev/spark-labs/05-api/src/main/scala/x/ProcessFiles.scala`**  
**=> And fix the TODO items**


## STEP 3: Compile the project

We will use `sbt` to build the project.  

**=> Inspect the `build.sbt` file**

The file will look follows:



**=> Kick off a build**  
(This will take a few minutes for the first time you run it)

```bash
    # be in the project root level directory
    $   cd   ~/dev/spark-labs/05-api

    $   sbt package

    # to do a clean rebuild use
    $  sbt clean package
```

Make sure there are no errors and there is output in `target` dir.

```bash
    $  ls -l   target/scala-2.12
```

You should see output like follows

```console
-rw-r--r-- 1 ubuntu ubuntu 12280 Oct 28 16:55 testapp_2.12-1.0.jar
```

`testapp_2.12-1.0.jar `  is our code compiled.


## STEP 3: Test Application in Local Master Mode

```bash
    $  cd  ~/dev/spark-labs/05-api

    $   ~/apps/spark/bin/spark-submit --master local[*]  \
          --class 'x.ProcessFiles' \
          target/scala-2.12/testapp_2.12-1.0.jar    \
          README.md
```

**==> Checkout the Shell UI (4040)**   

**==> Hit Enter key to terminate the program**

**==> Turn off the logs by sending logs by `2> logs`**   

```bash
    $   ~/apps/spark/bin/spark-submit --master local[*] \
          --class 'x.ProcessFiles' \
          target/scala-2.12/testapp_2.12-1.0.jar \
           README.md  2> logs 
```

**==> Try a few input files**
```bash
    $   ~/apps/spark/bin/spark-submit --master local[*]  \
          --class 'x.ProcessFiles' \
         target/scala-2.12/testapp_2.12-1.0.jar \
          /data/text/twinkle/*  2> logs
```


Now let's submit the application to Spark server

## STEP 4: Start Spark Server

```bash
    $  ~/apps/spark/sbin/start-all.sh
```

**=> Check the Spark Server UI at port 8080.**  
**=> Note the Spark master URL.**  

<img src="../assets/images/4.1b.png" style="border: 5px solid grey; max-width:100%;"/>


## STEP 5: Submit the application

Use the following command to submit the job

```bash
    $  cd  ~/dev/spark-labs/05-api

    # single README file
    $   ~/apps/spark/bin/spark-submit \
           --master MASTER_URL \
           --class 'x.ProcessFiles' \
           target/scala-2.12/testapp_2.12-1.0.jar \
            README.md   2> logs

    # multiple twinkle files
    $   ~/apps/spark/bin/spark-submit \
            --master MASTER_URL \
            --class 'x.ProcessFiles' \
            target/scala-2.12/testapp_2.12-1.0.jar \
            /data/text/twinkle/*  2> logs
```

* MASTER URL : substitute your spark master url
* for files you can try `README.md`

**=> Watch the console output**

It may look like this

```console
    15/04/15 15:41:51 INFO SparkUI: Started SparkUI at http://192.168.1.115:4040
    ...
    15/04/15 15:41:54 INFO DAGScheduler: Job 0 finished: count at ProcessFiles.scala:42, took 2.156893 s

    ### README.md : count (7) took 2,251.007000 ms
```

The lines starting with `###` are output from our program


## STEP 6:  Configuring logging

#### To quickly turn off the logging:
Redirect the logs as follows `  2> logs`.   
All logs will be sent to `logs` file.  
```bash
    $  ~/apps/spark/bin/spark-submit \
        --master MASTER_URL \
        --class 'x.ProcessFiles'  target/scala-2.12/testapp_2.12-1.0.jar    \
        <files to process>    2>  logs
```

#### Using log4j config
Spark by default logs at INFO level.  While there is a lot of useful information there, it can be quite verbose.

We have a `logging/log4j.properties` file.  Inspect this file

```bash
    $    cat   logging/log4j.properties
```


The file has following contents

```
# Set everything to be logged to the console
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Settings to quiet third party logs that are too verbose
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO

## set log levels for our loggers
## everything under 'x' package
#log4j.logger.x=INFO
## specific file
#log4j.logger.x.ProcessFiles=DEBUG
```



We provide `--driver-class-path logging/`  to spark-submit to turn off logging

Here is an example

```bash
    $   ~/apps/spark/bin/spark-submit \
         --master local[*] 
         --driver-class-path logging/   \
         --class 'x.ProcessFiles' \
         target/scala-2.12/testapp_2.12-1.0.jar \
         README.md
```
