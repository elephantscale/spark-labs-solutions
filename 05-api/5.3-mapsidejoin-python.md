

Lab 05.3 Mapside Join Python
=========================================================

### Overview
Broadcast Variables 
	 In cases if we need to join large tables(fact tables) with smaller ones(dimension tables),
instead of sending large table data over network, we can just broadcast the smaller table to all nodes to perform a map-side/broadcast join.

 Here we will be loading sample datasets of US state store location
 
### Depends On
 Should contain more than one node in the cluster for wide shuffle to happen

### Run time
20-30 mins


## STEP 1: Edit source file

Go to the project root directory

```bash
    $    cd <project-directory>
```


**=> Edit file : `python/mapsidejoin.py`**  
**=> And fix the TODO items**



## STEP 3: Test Application in Local Master Mode

```bash
    $   ~/apps/spark/bin/spark-submit  --master local[*]  python/mapsidejoin.py 
```

**==> Checkout the Shell UI (4040)**   

**==> Hit Enter key to terminate the program**

**==> Turn off the logs by sending logs by `2> logs` **   

```bash
    $   ~/apps/spark/bin/spark-submit --master local[*]  python/mapsidejoin.py  2> logs
```


Now let's submit the application to Spark server

## STEP 4: Start Spark Server

```bash
    $  ~/apps/spark/sbin/start-all.sh
```

**=> Check the Spark Server UI at port 8080.**  
**=> Note the Spark master URL.**  

<img src="../assets/images/4.1b.png" style="border: 5px solid grey; max-width:100%;"/>


## STEP 5: Submit the application

Use the following command to submit the job

```bash
    $  cd  ~/dev/spark-labs/05-api

    $   ~/apps/spark/bin/spark-submit --master MASTER URL python/mapsidejoin.py  2>logs
```

* MASTER URL : substitute your spark master url

**=> Watch the console output**

It may look like this

```console
###Number of stores in each US region :
+-------------+-----+
|census_region|count|
+-------------+-----+
|      Midwest|   77|
|        South|  117|
|         West|  222|
|    Northeast|   59|
+-------------+-----+

```
The lines starting with `###` are output from our program


## STEP 6:  Configuring logging

#### To quickly turn off the logging:
Redirect the logs as follows `  2> logs`.   
All logs will be sent to `logs` file.  
```bash
    $  ~/apps/spark/bin/spark-submit --master MASTER_URL   python/mapsidejoin.py 2>  logs
```

#### Using log4j config
Spark by default logs at INFO level.  While there is a lot of useful information there, it can be quite verbose.

We have a `logging/log4j.properties` file.  Inspect this file

```bash
    $    cat   logging/log4j.properties
```


The file has following contents

```
# Set everything to be logged to the console
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Settings to quiet third party logs that are too verbose
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO

## set log levels for our loggers
## everything under 'x' package
#log4j.logger.x=INFO
## specific file
#log4j.logger.spark.basic.MapSideJoin=DEBUG
```



We provide `--driver-class-path logging/`  to spark-submit to turn off logging

Here is an example

```bash
    $   ~/apps/spark/bin/spark-submit --master local[*]  --driver-class-path logging/ python/mapsidejoin.py 
```
